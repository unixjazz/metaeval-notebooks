{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8RTc2PmnX-v"
   },
   "source": [
    "# \"Openness\" in LLM benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGW7vfRkrqHe"
   },
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGW7vfRkrqHe",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The question of \"openness\" in AI research is wide open, marked with controversies regarding how and what should be made available.\n",
    "\n",
    "This issue in large language modeling (LLM) research is particularly heated, given the unparalleled commercial interest, but also, and most importanly, its concrete and potential impact on several areas of endeavor from the arts to the sciences. What does it mean to \"open\" a large language model application or the research and development process of an LLM? What \"levels of openness\" are perceived to be necessary by practitioners across the data \"pipeline\"---from data collection, preparation, and training to post-training, fine-tuning, and deployment? What is not \"open\" but forcibly made open into foundational models of so-called \"open\" AI, reigniting the debate about \"fair use\" and \"intellectual property\" rights? \n",
    "\n",
    "These questions are just the tip of the proverbial iceberg of AI systems. They concern an emergent problem space where new understandings and practices of \"openness\" are being portrayed in the context of AI research and development. \n",
    "\n",
    "These questions are part of an emergent problem-space concerning the question of participation in the present and future of computing. Given the new prevalance of private over publicly-funded research, the future of open AI scholarship is currently being put to test. And the question of \"openness\" takes us to the key concerns of research and development in AI for the examination of its sociotechnical conditions, processes, and resources.\n",
    "\n",
    "In this notebook, we will bracket the sociotechnical complexity of AI systems to focus on the question of \"openness\" of benchmarking practices of model evaluation. We will do so with a focus on one of the most popular \"open AI\" systems, Meta Llama v3. We will describe the benchmarking of a large language model (LLM) to discuss the affordances, but also the foreclosures of \"openness.\" We will operate with the distinction between \"open AI\" (as an industry practice of releasing documentation and \"open weights\") versus \"Open Source AI\" (as defined and vetted by the Open Source Initiative), but will take a naive approach to \"openness\" through a simple exercise: we will test whether we can reproduce a benchmark of an \"open\" foundation model family without reference (initially) to any particular understanding or vetted definition of \"openness\" (whether de facto or de jure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most controversial aspects in the debate about openness, accessibility, and transparency concerns the question of training data. Where is it obtained? How is it obtained? What is the licensing status of the data obtained? Who owns the copyright of the training data? Who benefits and who does not in the process?\n",
    "\n",
    "This is an area where a shroud of mystery and secrecy exists. Most of what is currently known was obtained through court documents and leked information from internal processes of big tech companies with LLM products.\n",
    "\n",
    "On this topic, open AI systems usually do no disclose as much as necessary for an informed debate about the ethics of LLM research. For example, in the whitepaper that presents the technical aspects of Meta Llama 3, \"Heards of Models,\" the initial amount of data is not detailed, nor are the details of data collection and processing. Descriptions of the data are vague. They refer to a \"a large, multilingual text corpus\" of 15.6 trillion tokens. We are informed the quality is substantially better than the previous models, but also that \"much of the data is obtained from the web.\" In their terms, \"a new mix of publicly available online data.\" Personally identifiable information (PII) and adult content is claimed to be removed from the final training set.\n",
    "\n",
    "To find more information about the corpus, we cannot rely on official documentation. We need to collect information through informal ways. We find, for example, that Llama has, at least, a known combination of sources, such as:\n",
    "- Common Crawl (~3 million web pages)\n",
    "- Colossal Clean Crawled Corpus (C4, based on Common Crawl)\n",
    "- Books3 collection (191,000 pirated books)\n",
    "- Project Gutenberg and Wikipedia\n",
    "- ArXiV, Github and Stack Overflow\n",
    "  Corporate news outlets (i.e. NYT)\n",
    "\n",
    "Copyrighted material can be found in most of these datasets, but claims of fair use have been used in the past (for Common Crawl) and prevailed, more recently, in a court case against copyright violation against Meta (in its training of Llama). Another fundamental aspect is that the conditions of production of the language practices that are \"scrapped\" from the web are all erased through the process of data cleaning and preparation for feeding into a \"tokenizer\" that divides up the corpus into elementary tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Public Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Open Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI research was an arcane and fairly insular domain of scholarship before it become a profitable industry. As such, it inherited the moral economy of contemporary academic scholarship with the incorporation of Open Access and Open Science practices. There is a common practice among practitioners, even newcomers, of preparing \"preprints\" that are circulated before and after important academic conferences. As part of the \"open science\" ethos, papers often include code and, at times, data (for training, post-training or benchmarking).\n",
    "\n",
    "There is also an important tendency that extends from the incorporation of Open Source development practices in the tech industry to the domain of AI research, which is the incorporation of OS development practices and tools into the AI research process. We find, for example, repositories for projects with papers and code. We find frameworks and tools that are widely distributed, included key frameworks and tools that are developed and made available using open licenses by big industry players (such as Meta, OpenAI, Google, and IBM). There is a general orientation, we could say, toward a practice of sharing tools for AI research and development. And this tendency is usually referred to by practitioners as the reason why the field of AI reseach is moving so fast in terms of its results, but also in terms of investment and interest.\n",
    "\n",
    "One of the open tools of interest in the space of bencharking is LLM Harness, developed by Eleuther AI (a research collective and non-profit that advocates for \"Open AI research\"). LLM Harness was created to fullfil an important need in the domain of LLM research: address the problem of reproduciblity where new research would present the improvements on the state-of-the art, but not offer code and data for independent researchers reproduce the results.\n",
    "\n",
    "How reproducible is a benchmark? (Or, better, how reproducible should it be, given all the challenges of reproducibility?) The answer to this question is, interestingly, far from straight forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language model benchmarking is one of the most discussed, yet one of the most problematized aspects of current LLM research. Many practitioners are dismissive of the industry reliance on benchmarks, given the problems that have been identified in the literature regarding the problems of reproducibility, such as: \n",
    "* Ambiguous / polysemic preparation and interpretation of prompts\n",
    "* Difference in GPU hardware\n",
    "* Benchmark dataset being incorporated by the training data of the model (or similar tasks from the benchmark)\n",
    "* Poorly designed benchmarks (that have more than one answer to their questions)\n",
    "\n",
    "The technical evaluation of Llama v3 was conducted in three steps. First, we identified the models to study based on 1) how active their community was online; 2) how performant they were in previous benchmarks; and 3) how available (and accepted) their models and tools were\n",
    "for the research community. We identified in the Llama family of models a good candidate for being at the center of the controversy regarding \"open\" versus \"open source\" AI. Second, we prepared the environmental and datasets to run the benchmarks. Third and last, we ran evaluations and documented the results, while saving the outputs to see how the models actually responded to automated tasks.\n",
    "\n",
    "For testing whether the benchmark results from Meta could be reproduced, we adopted one of the most popular benchmarks that is also used and reported first by the company when sharing their progress on LLM training. The MMLU (Measuring Massive Multitask Language Understanding) benchmark consists in a series of questions of general knowledge, organized in a dataset that contains responses (annotations) to multiple choice questions. The MMLU dataset is widely recognized in the literature as a robust tool for evaluating language models, covering 116,000 multiple-choice questions distributed. However, it's robustness is also object of strong critique. Gema et al. (2025) in their article \"Are We Done with MMLU?\" describes serious problems with MMLU that compromise its usefulness as an evaluation for comparison across models. They report, for example, that around 67% of the answers for the subject of virology are wrong. Overall, they have identified several problems in the benchmark, including: ambiguous questions, questions with reference to objects that are not part of the benchmark (wrongly extracted from online sources), questions that have more than one answer, as well as biased answers in the legal subject that do not include jurisdiction (but assume the US as default). For our purposes in testing the \"openness\" of evaluation tools, however, MMLU is the most adequate tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Running the benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When attempting to use Llama models, we encounter the first restriction: they are \"gated models\" for which users needs to register with Meta to have access, after declaring that the terms of the \"META LLAMA 3 COMMUNITY LICENSE AGREEMENT\" are accepted.\n",
    "\n",
    "This is the message a user will encounter if not registered or not accepted to register:\n",
    "\n",
    "```\n",
    "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
    "Access to model meta-llama/Meta-Llama-3-8B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B to ask for access.\n",
    "```\n",
    "\n",
    "Interestingly, one of the biggest number of \"orphaned tickets\" opened on the repository of the model (meta/meta-llama) concerns the denial of access to users. Many users are turned down without explanation.\n",
    "\n",
    "Below, we will run the MMLU benchmark on models of 3 sizes (1B, 3B and 8B parameters) and report the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run this for the first time to install the requirements:\n",
    "#\n",
    "#!pip install -r https://huggingface.co/flunardelli/llm-metaeval/raw/main/requirements.txt\n",
    "#\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "#\n",
    "# We need to pass a HF token to continue\n",
    "#\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xP0cC_sHih7C"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here we declare the parameters of the eval\n",
    "#\n",
    "YAML_mmlu_en_us_string = \"\"\"\n",
    "task: mmlu_all\n",
    "dataset_path: cais/mmlu\n",
    "dataset_name: all\n",
    "description: \"MMLU dataset\"\n",
    "test_split: test\n",
    "fewshot_split: dev\n",
    "fewshot_config:\n",
    "  sampler: first_n\n",
    "num_fewshot: 5\n",
    "output_type: multiple_choice\n",
    "doc_to_text: \"{{question.strip()}}\\nA. {{choices[0]}}\\nB. {{choices[1]}}\\nC. {{choices[2]}}\\nD. {{choices[3]}}\\nAnswer:\"\n",
    "doc_to_choice: [\"A\", \"B\", \"C\", \"D\"]\n",
    "doc_to_target: answer\n",
    "metric_list:\n",
    "  - metric: acc\n",
    "    aggregation: mean\n",
    "    higher_is_better: true\n",
    "\"\"\"\n",
    "with open(\"mmlu_en_us.yaml\", \"w\") as f:\n",
    "    f.write(YAML_mmlu_en_us_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here is the result for MMLU, accuracy test of the language model with 1B parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IzP5nyP0Gwk8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07:16:50:07,114 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2025-10-07:16:50:07,114 INFO     [__main__.py:303] Including path: ./\n",
      "2025-10-07:16:50:11,812 INFO     [__main__.py:376] Selected Tasks: ['mmlu_all']\n",
      "2025-10-07:16:50:11,812 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-10-07:16:50:11,813 INFO     [evaluator.py:201] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B-Instruct'}\n",
      "2025-10-07:16:50:11,979 INFO     [huggingface.py:131] Using device 'cuda:0'\n",
      "2025-10-07:16:50:12,888 INFO     [huggingface.py:368] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "2025-10-07:16:50:13,894 INFO     [evaluator.py:221] Using cache at cache_rank0.db\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 48fb1754-9fa8-4193-b8e8-9b67fbc8f536)')' thrown while requesting HEAD https://huggingface.co/datasets/cais/mmlu/resolve/c30699e8356da336a370243923dbaf21066bb9fe/.huggingface.yaml\n",
      "2025-10-07:16:50:24,837 WARNING  [_http.py:321] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 48fb1754-9fa8-4193-b8e8-9b67fbc8f536)')' thrown while requesting HEAD https://huggingface.co/datasets/cais/mmlu/resolve/c30699e8356da336a370243923dbaf21066bb9fe/.huggingface.yaml\n",
      "Retrying in 1s [Retry 1/5].\n",
      "2025-10-07:16:50:24,837 WARNING  [_http.py:330] Retrying in 1s [Retry 1/5].\n",
      "2025-10-07:16:50:27,297 INFO     [task.py:415] Building contexts for mmlu_all on rank 0...\n",
      "100%|████████████████████████████████████| 14042/14042 [00:58<00:00, 238.03it/s]\n",
      "2025-10-07:16:51:26,573 INFO     [evaluator.py:496] Running loglikelihood requests\n",
      "2025-10-07:16:51:26,581 INFO     [model.py:257] Loading 'loglikelihood' responses from cache 'cache_rank0.db' where possible...\n",
      "Checking cached requests: 100%|█████████| 56168/56168 [00:06<00:00, 8232.83it/s]\n",
      "2025-10-07:16:51:33,404 INFO     [model.py:281] Cached requests: 56168, Requests remaining: 0\n",
      "2025-10-07:16:51:36,483 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2025-10-07:16:51:36,489 INFO     [evaluation_tracker.py:287] Saving per-sample results for: mmlu_all\n",
      "hf (pretrained=meta-llama/Llama-3.2-1B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1\n",
      "| Tasks  |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------|-------|------|-----:|------|---|-----:|---|-----:|\n",
      "|mmlu_all|Yaml   |none  |     5|acc   |↑  |0.3431|±  | 0.004|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Llama-3.2-1B-Instruct \\\n",
    "  --include_path ./ \\\n",
    "  --tasks mmlu_all \\\n",
    "  --output output/mmlu/ \\\n",
    "  --use_cache cache \\\n",
    "  --device cuda:0 \\\n",
    "  --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here is the result for MMLU, accuracy test of the language model with 3B parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oIACOAhDW5ow",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07:16:51:41,421 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2025-10-07:16:51:41,421 INFO     [__main__.py:303] Including path: ./\n",
      "2025-10-07:16:51:46,094 INFO     [__main__.py:376] Selected Tasks: ['mmlu_all']\n",
      "2025-10-07:16:51:46,095 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-10-07:16:51:46,095 INFO     [evaluator.py:201] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-3B-Instruct'}\n",
      "2025-10-07:16:51:46,261 INFO     [huggingface.py:131] Using device 'cuda:0'\n",
      "2025-10-07:16:51:47,137 INFO     [huggingface.py:368] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.59it/s]\n",
      "2025-10-07:16:51:48,509 INFO     [evaluator.py:221] Using cache at cache2_rank0.db\n",
      "2025-10-07:16:51:50,986 INFO     [task.py:415] Building contexts for mmlu_all on rank 0...\n",
      "100%|████████████████████████████████████| 14042/14042 [00:57<00:00, 242.94it/s]\n",
      "2025-10-07:16:52:49,051 INFO     [evaluator.py:496] Running loglikelihood requests\n",
      "2025-10-07:16:52:49,059 INFO     [model.py:257] Loading 'loglikelihood' responses from cache 'cache2_rank0.db' where possible...\n",
      "Checking cached requests: 100%|████████| 56168/56168 [00:03<00:00, 17671.04it/s]\n",
      "2025-10-07:16:52:52,238 INFO     [model.py:281] Cached requests: 0, Requests remaining: 56168\n",
      "Running loglikelihood requests: 100%|█████| 56168/56168 [09:55<00:00, 94.25it/s]\n",
      "2025-10-07:17:03:39,783 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2025-10-07:17:03:39,790 INFO     [evaluation_tracker.py:287] Saving per-sample results for: mmlu_all\n",
      "hf (pretrained=meta-llama/Llama-3.2-3B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1\n",
      "| Tasks  |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------|-------|------|-----:|------|---|-----:|---|-----:|\n",
      "|mmlu_all|Yaml   |none  |     5|acc   |↑  |0.5319|±  |0.0042|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct \\\n",
    "  --include_path ./ \\\n",
    "  --tasks mmlu_all \\\n",
    "  --output output/mmlu/ \\\n",
    "  --use_cache cache2 \\\n",
    "  --device cuda:0 \\\n",
    "  --log_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here is the result for MMLU, accuracy test of the language model with 8B parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFFYPzBIYGf7"
   },
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "#torch.cuda.empty_cache()\n",
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Meta-Llama-3-8B,revision=62bd457b6fe961a42a631306577e622c83876cb6,dtype=float16 \\\n",
    "  --include_path ./ \\\n",
    "  --tasks mmlu_all \\\n",
    "  --output output/mmlu/ \\\n",
    "  --use_cache cache3 \\\n",
    "  --device cuda:0 \\\n",
    "  --log_samples \\\n",
    "  --batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUTPHnV0kMB1"
   },
   "source": [
    "### 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the result of our benchmarking exercise and what does it tell us about the \"openness\" of benchmarks themselves for \"open\" AI systems? How did our results compare with those of Meta?\n",
    "\n",
    "For the MMLU benchmark, all 57 available subgroups or topics were included, using the arithmetic mean of accuracy to group the results. The few-shot strategy (with 5 examples) was applied in the tests to improve overall performance. Among all the models we evaluated, Meta-Llama-3 with 8 billion parameters achieved the highest accuracy in the MMLU benchmark (57.54%), surpassing the 3 billion (53.23%) and 1\n",
    "billion (34.13%) models, evidencing the influence of model size on test performance. On the other hand, the increase in accuracy was less significant when comparing the Meta-Llama-3B and 8B models, reflecting that, although relevant, the impact of model size may present limits\n",
    "for performance improvements---contrary to common held assumption that scaling is comensurate with better model performance. \n",
    "\n",
    "| Model             | LLM-Harness | Meta |\n",
    "| :---------------- | :---------: | ---: |\n",
    "| Llama 3.2 1B      | 34.13       | 49.3 |\n",
    "| Llama 3.2 3B      | 53.23       | 63.4 |\n",
    "| Llama 3.2 8B      | 57.54       | ?    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTHORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text / Analysis: LF Murillo and Sara E. Berger;\n",
    "* Benchmarking: Fernando Lunardelli, CC-BY-SA 10-06-2025."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
